{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Highest Cost Orders `Shopify`\n",
        "\n",
        "Find the customers with the highest daily total order cost between 2019-02-01 and 2019-05-01. If a customer had more than one order on a certain day, sum the order costs on a daily basis. Output each customer's first name, total cost of their items, and the date.\n",
        "\n",
        "\n",
        "For simplicity, you can assume that every first name in the dataset is unique."
      ],
      "metadata": {
        "id": "B1fMDTDpNcff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "orders[orders['order_date'].between('2019-02-01','2019-05-01')].merge(customers,left_on='cust_id',right_on='id').groupby(['first_name','order_date'])['total_order_cost'].sum().reset_index().sort_values(by='total_order_cost',ascending=False).head(2)"
      ],
      "metadata": {
        "id": "c7pafYSFNb19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Products `Tesla`\n",
        "\n",
        "Calculate the net change in the number of products launched by companies in 2020 compared to 2019. Your output should include the company names and the net difference.\n",
        "(Net difference = Number of products launched in 2020 - The number launched in 2019.)"
      ],
      "metadata": {
        "id": "YdamknasN3ld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLG4IYDRNFHi"
      },
      "outputs": [],
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# car_launches.groupby(['year','company_name'],as_index=False)['product_name'].count()\n",
        "car2019 = car_launches.query('year == 2019').groupby('company_name')['product_name'].nunique()\n",
        "car2020 = car_launches.query('year == 2020').groupby('company_name')['product_name'].nunique()\n",
        "\n",
        "(car2020 - car2019).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Users By Average Session Time `Meta`\n",
        "\n",
        "Calculate each user's average session time, where a session is defined as the time difference between a page_load and a page_exit. Assume each user has only one session per day. If there are multiple page_load or page_exit events on the same day, use only the latest page_load and the earliest page_exit, ensuring the page_load occurs before the page_exit. Output the user_id and their average session time."
      ],
      "metadata": {
        "id": "EutMdRm1N9k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "facebook_web_log['day']  = facebook_web_log['timestamp'].dt.day\n",
        "\n",
        "df = facebook_web_log[~facebook_web_log['action'].str.contains('scroll')]\n",
        "\n",
        "df = df.groupby(['user_id','day','action'],as_index=False)['timestamp'].max()\n",
        "\n",
        "df = df.pivot_table(index=['user_id','day'],columns='action',values='timestamp').reset_index().dropna()\n",
        "\n",
        "df['duration'] = df['page_exit'] - df['page_load']\n",
        "df.groupby('user_id',as_index=False)['duration'].mean()"
      ],
      "metadata": {
        "id": "KgybQ2PQOAt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acceptance Rate By Date `Meta`\n",
        "\n",
        "Calculate the friend acceptance rate for each date when friend requests were sent. A request is sent if action = sent and accepted if action = accepted. If a request is not accepted, there is no record of it being accepted in the table. The output will only include dates where requests were sent and at least one of them was accepted, as the acceptance rate can only be calculated for those dates. Show the results ordered from the earliest to the latest date."
      ],
      "metadata": {
        "id": "NSDV0fIkOELn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = fb_friend_requests.pivot_table(index=['user_id_sender','user_id_receiver'],columns='action',values='date').reset_index()\n",
        "\n",
        "sent = pd.DataFrame(columns=['date','sent'])\n",
        "\n",
        "sent[['date','sent']] = df['sent'].value_counts().reset_index()\n",
        "\n",
        "accepted = df.groupby('sent',as_index=False)['accepted'].count()\n",
        "\n",
        "final = accepted.merge(sent,left_on='sent',right_on='date')\n",
        "\n",
        "final['acceptance_rate'] = final['accepted']/final['sent_y']\n",
        "\n",
        "final[['date','acceptance_rate']]"
      ],
      "metadata": {
        "id": "Xgfla1MqOGwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding User Purchases `Amazon`\n",
        "\n",
        "Identify returning active users by finding users who made a second purchase within 1 to 7 days after their first purchase. Ignore same-day purchases. Output a list of these user_ids.\n"
      ],
      "metadata": {
        "id": "tc-uV02KOKSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# amazon_transactions[['shifted_users','shifted_created']] =\n",
        "df = amazon_transactions.sort_values(by=['user_id','created_at'])\n",
        "df[['shifted_users','shifted_created']] = df[['user_id','created_at']].shift(1)\n",
        "df = df[(df['user_id'] == df['shifted_users']) | (df['shifted_users'].isna())]\n",
        "\n",
        "df['delta'] = ((df['created_at'] - df['shifted_created'])/3600)/24\n",
        "\n",
        "df['delta'] = df['delta'].dt.seconds\n",
        "\n",
        "df = df.query('delta != 0')\n",
        "\n",
        "df = df.dropna().drop_duplicates(subset='user_id')\n",
        "df[df['delta'] < 8]['user_id']"
      ],
      "metadata": {
        "id": "RsbZuwUGORVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Premium vs Freemium `Microsoft`\n",
        "\n",
        "Find the total number of downloads for paying and non-paying users by date. Include only records where non-paying customers have more downloads than paying customers. The output should be sorted by earliest date first and contain 3 columns date, non-paying downloads, paying downloads. Hint: In Oracle you should use \"date\" when referring to date column (reserved keyword)."
      ],
      "metadata": {
        "id": "6mwIkAPROWvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = ms_user_dimension.merge(ms_acc_dimension,on='acc_id').merge(ms_download_facts,on='user_id').groupby(['date','paying_customer'],as_index=False)['downloads'].sum()\n",
        "\n",
        "df = df.pivot_table(index='date',columns='paying_customer',values='downloads').reset_index()\n",
        "\n",
        "df.query('no > yes')"
      ],
      "metadata": {
        "id": "JgwgiDIvOYmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Risky Projects `Linkedin`\n",
        "\n",
        "You are given a set of projects and employee data. Each project has a name, a budget, and a specific duration, while each employee has an annual salary and may be assigned to one or more projects for particular periods. The task is to identify which projects are overbudget. A project is considered overbudget if the prorated cost of all employees assigned to it exceeds the project’s budget.\n",
        "\n",
        "\n",
        "To solve this, you must prorate each employee's annual salary based on the exact period they work on a given project, relative to a full year. For example, if an employee works on a six-month project, only half of their annual salary should be attributed to that project. Sum these prorated salary amounts for all employees assigned to a project and compare the total with the project’s budget.\n",
        "\n",
        "\n",
        "Your output should be a list of overbudget projects, where each entry includes the project’s name, its budget, and the total prorated employee expenses for that project. The total expenses should be rounded up to the nearest dollar. Assume all years have 365 days and disregard leap years."
      ],
      "metadata": {
        "id": "YO1KfAjXOeTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "\n",
        "# Start writing coded\n",
        "linkedin_projects['duration'] = ((linkedin_projects['end_date'] - linkedin_projects['start_date']).dt.days)\n",
        "\n",
        "df = linkedin_projects.merge(linkedin_emp_projects,left_on='id',right_on='project_id').drop(columns='id').merge(linkedin_employees,left_on='emp_id',right_on='id')\n",
        "\n",
        "df['prorated'] = (df['salary']/365)*(df['duration'])\n",
        "\n",
        "df = df.groupby('title',as_index=False).agg(\n",
        "    {\n",
        "        'budget' : 'max',\n",
        "        'prorated' : 'sum'\n",
        "    })\n",
        "\n",
        "df = df.query('prorated > budget')\n",
        "\n",
        "df['prorated'] = df['prorated'].apply(lambda x : ceil(x))\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "2dsTWKssOhVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity Rank `Google`\n",
        "\n",
        "Find the email activity rank for each user. Email activity rank is defined by the total number of emails sent. The user with the highest number of emails sent will have a rank of 1, and so on. Output the user, total emails, and their activity rank.\n",
        "\n",
        "\n",
        "•\tOrder records first by the total emails in descending order.\n",
        "•\tThen, sort users with the same number of emails in alphabetical order by their username.\n",
        "•\tIn your rankings, return a unique value (i.e., a unique rank) even if multiple users have the same number of emails.\n"
      ],
      "metadata": {
        "id": "2dR_kuKkOlKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = google_gmail_emails.groupby('from_user',as_index=False)['to_user'].count()\n",
        "\n",
        "df = df.sort_values(by=['to_user','from_user'],ascending=[False,True])\n",
        "\n",
        "df['rank'] = df['to_user'].rank(ascending=False,method='first')\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "1pBQvB9ROnXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processed Ticket Rate By Type `Meta`\n",
        "\n",
        "Find the processed rate of tickets for each type. The processed rate is defined as the number of processed tickets divided by the total number of tickets for that type. Round this result to two decimal places."
      ],
      "metadata": {
        "id": "cE9RM1bZOskv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = facebook_complaints.groupby('type',as_index=False).agg(\n",
        "    {'processed' : 'sum',\n",
        "    'complaint_id' : 'count'\n",
        "    }\n",
        "    )\n",
        "\n",
        "df['processed_rate'] = df['processed']/df['complaint_id']\n",
        "\n",
        "df[['type','processed_rate']]"
      ],
      "metadata": {
        "id": "ISUcWKKwOxNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customer Revenue In March `Amazon`\n",
        "\n",
        "Calculate the total revenue from each customer in March 2019. Include only customers who were active in March 2019. An active user is a customer who made at least one transaction in March 2019.\n",
        "\n",
        "\n",
        "Output the revenue along with the customer id and sort the results based on the revenue in descending order."
      ],
      "metadata": {
        "id": "_OgCFYP5O2vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "(\n",
        "    orders\n",
        "    .query('order_date >= \"2019-03\" & order_date < \"2019-04\"')\n",
        "    .groupby('cust_id',as_index=False)['total_order_cost'].sum()\n",
        "    .sort_values(by='total_order_cost',ascending=False)\n",
        ")"
      ],
      "metadata": {
        "id": "5NgiU2KZO41E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count Occurrences Of Words In Drafts `Google`\n",
        "\n",
        "Find the number of times each word appears in the contents column across all rows in the google_file_store dataset. Output two columns: word and occurrences."
      ],
      "metadata": {
        "id": "wf8iF9HmO77p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "google_file_store['contents'] = google_file_store['contents'].str.split(' ')\n",
        "dic = {}\n",
        "for i in google_file_store['contents'].sum():\n",
        "    i = i.replace('.','').replace(',','').lower()\n",
        "    if i in dic:\n",
        "        dic[i] += 1\n",
        "    else:\n",
        "        dic[i] = 1\n",
        "\n",
        "df = pd.DataFrame(columns=['word','occurences'])\n",
        "\n",
        "df['word'] = dic.keys()\n",
        "\n",
        "df['occurences'] = dic.values()\n",
        "df.sort_values(by='occurences',ascending=False)"
      ],
      "metadata": {
        "id": "OANZA1vyO-3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Titanic Survivors and Non-Survivors `Tesla`\n",
        "\n",
        "Make a report showing the number of survivors and non-survivors by passenger class. Classes are categorized based on the pclass value as:\n",
        "\n",
        "\n",
        "•\tFirst class: pclass = 1\n",
        "•\tSecond class: pclass = 2\n",
        "•\tThird class: pclass = 3\n",
        "\n",
        "\n",
        "Output the number of survivors and non-survivors by each class."
      ],
      "metadata": {
        "id": "HSxmFi9qPB4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "\n",
        "titanic.pivot_table(index='survived',columns='pclass',values='ticket',aggfunc='count').reset_index()"
      ],
      "metadata": {
        "id": "X0xLAW2bPE70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Highest Salary `Dropbox`\n",
        "\n",
        "Find the second highest salary of employees."
      ],
      "metadata": {
        "id": "0FipUNOePIAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "employee['salary'].sort_values(ascending=False).iloc[1]"
      ],
      "metadata": {
        "id": "5U9s2_j2PLVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Employee and Manager Salaries `Walmart`\n",
        "\n",
        "\n",
        "Find employees who are earning more than their managers. Output the employee's first name along with the corresponding salary."
      ],
      "metadata": {
        "id": "jj3vusBfPOqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "employee.merge(employee,left_on='manager_id',right_on='id').query('salary_x > salary_y')[['first_name_x','salary_x']]"
      ],
      "metadata": {
        "id": "6XhRTgoXPW3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Highest Salary In Department `Asana`\n",
        "\n",
        "Find the employee with the highest salary per department.\n",
        "Output the department name, employee's first name along with the corresponding salary."
      ],
      "metadata": {
        "id": "cgGtn5tKPaLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "# Start writing code\n",
        "for i,j in employee.groupby('department'):\n",
        "    salary = j['salary'].max()\n",
        "    name = j.query(f'salary == {salary}')['first_name'].values[0]\n",
        "    data.append({'department' : i,'name' : name,'salary':salary})\n",
        "\n",
        "pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "HqhLf8VNPcY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Highest Target Under Manager `Salesforce`\n",
        "\n",
        "Identify the employee(s) working under manager manager_id=13 who have achieved the highest target. Return each such employee’s first name alongside the target value. The goal is to display the maximum target among all employees under manager_id=13 and show which employee(s) reached that top value."
      ],
      "metadata": {
        "id": "rNgEqLZXPf1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = salesforce_employees.query('manager_id == 13')\n",
        "\n",
        "df[df['target'] == df['target'].max()][['first_name','target']]"
      ],
      "metadata": {
        "id": "Qg89iWQCPhub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Largest Olympics `ESPN`\n",
        "\n",
        "Find the Olympics with the highest number of unique athletes. The Olympics game is a combination of the year and the season, and is found in the games column. Output the Olympics along with the corresponding number of athletes. The id column uniquely identifies an athlete."
      ],
      "metadata": {
        "id": "9GR8gF9DPky1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "olympics_athletes_events['oly'] = olympics_athletes_events['year'].astype(str) + ' ' + olympics_athletes_events['season'].astype(str)\n",
        "\n",
        "olympics_athletes_events.groupby('oly',as_index=False)['id'].nunique().sort_values(by='id',ascending=False).head(1)"
      ],
      "metadata": {
        "id": "5BBTbEEDPnQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top Businesses With Most Reviews `Yelp`\n",
        "\n",
        "Find the top 5 businesses with most reviews. Assume that each row has a unique business_id such that the total reviews for each business is listed on each row. Output the business name along with the total number of reviews and order your results by the total reviews in descending order."
      ],
      "metadata": {
        "id": "T0mJudkcPqny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "yelp_business.sort_values(by='review_count',ascending=False).head(5)[['name','review_count']]"
      ],
      "metadata": {
        "id": "di0QC3IlPtIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reviews of Categories `Yelp`\n",
        "\n",
        "Calculate number of reviews for every business category. Output the category along with the total number of reviews. Order by total reviews in descending order."
      ],
      "metadata": {
        "id": "TtDgwMF1PwPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "expnd = yelp_business['categories'].str.split(';',expand=True)\n",
        "df = pd.concat([yelp_business,expnd],axis=1)\n",
        "\n",
        "df.melt(id_vars=yelp_business.columns)[['value','review_count']].groupby('value',as_index=False)['review_count'].sum().sort_values(by='review_count',ascending=False)"
      ],
      "metadata": {
        "id": "hjvvPPDvPyYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top Cool Votes `Yelp`\n",
        "\n",
        "Find the review_text that received the highest number of  cool votes.\n",
        "Output the business name along with the review text with the highest number of cool votes."
      ],
      "metadata": {
        "id": "S1CkmNJOP1v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = yelp_reviews.groupby('business_name')['cool'].sum().sort_values(ascending=False).reset_index()\n",
        "df['ranks'] = df['cool'].rank(ascending=False,method='dense')\n",
        "yelp_reviews[yelp_reviews['business_name'].isin(df.query('ranks == 1')['business_name'].values)][['business_name','review_text']]"
      ],
      "metadata": {
        "id": "eaLJU2rqP306"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Income By Title and Gender `City of San Francisco`\n",
        "\n",
        "Find the average total compensation based on employee titles and gender. Total compensation is calculated by adding both the salary and bonus of each employee. However, not every employee receives a bonus so disregard employees without bonuses in your calculation. Employee can receive more than one bonus.\n",
        "Output the employee title, gender (i.e., sex), along with the average total compensation."
      ],
      "metadata": {
        "id": "YeBHg-r_P694"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = sf_employee.set_index('id').join(sf_bonus.groupby(['worker_ref_id'])['bonus'].sum(),how='right')\n",
        "\n",
        "df['com'] = df['salary'] + df['bonus']\n",
        "\n",
        "df.groupby(['employee_title','sex'])['com'].mean().reset_index()"
      ],
      "metadata": {
        "id": "AntomvFQP9F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matching Similar Hosts and Guests `Airbnb`\n",
        "\n",
        "Find matching hosts and guests pairs in a way that they are both of the same gender and nationality.\n",
        "Output the host id and the guest id of matched pair."
      ],
      "metadata": {
        "id": "dBcqq4v-QAUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "airbnb_hosts.drop_duplicates().merge(airbnb_guests,left_on=['nationality','gender'],right_on=['nationality','gender'])[['host_id','guest_id']]"
      ],
      "metadata": {
        "id": "0g2UzC0bQEFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the percentage of shipable orders `Google`\n",
        "\n",
        "Find the percentage of shipable orders.\n",
        "Consider an order is shipable if the customer's address is known."
      ],
      "metadata": {
        "id": "5XQXpu3BQPgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = orders.merge(customers,left_on='cust_id',right_on='id')\n",
        "\n",
        "int((df.dropna().shape[0]/df.shape[0])*100)"
      ],
      "metadata": {
        "id": "B_hjrHSIQRz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spam Posts `Meta`\n",
        "\n",
        "Calculate the percentage of spam posts in all viewed posts by day. A post is considered a spam if a string \"spam\" is inside keywords of the post. Note that the facebook_posts table stores all posts posted by users. The facebook_post_views table is an action table denoting if a user has viewed a post."
      ],
      "metadata": {
        "id": "L0wVqsXVQTIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "facebook_posts['is_spam'] = facebook_posts['post_keywords'].apply(lambda x : 'spam' in x)\n",
        "\n",
        "spam = facebook_posts.groupby('post_date',as_index=False)['is_spam'].sum()\n",
        "\n",
        "views = facebook_posts.merge(facebook_post_views,on='post_id').groupby('post_date',as_index=False)['post_id'].nunique()\n",
        "\n",
        "df = spam.merge(views,on='post_date')\n",
        "\n",
        "df['spam_share'] = (df['is_spam']/df['post_id'])*100\n",
        "\n",
        "df[['post_date','spam_share']]"
      ],
      "metadata": {
        "id": "2aZ67w3gQXAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apple Product Counts `Apple`\n",
        "\n",
        "We’re analyzing user data to understand how popular Apple devices are among users who have performed at least one event on the platform. Specifically, we want to measure this popularity across different languages. Count the number of distinct users using Apple devices —limited to \"macbook pro\", \"iphone 5s\", and \"ipad air\" — and compare it to the total number of users per language.\n",
        "\n",
        "\n",
        "Present the results with the language, the number of Apple users, and the total number of users for each language. Finally, sort the results so that languages with the highest total user count appear first."
      ],
      "metadata": {
        "id": "CVs3iK1dQaFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = playbook_events.merge(playbook_users,on='user_id')\n",
        "\n",
        "total_users = df.groupby(['language'],as_index=False)['user_id'].nunique()\n",
        "\n",
        "apple_users = df[df['device'].isin(['macbook pro','iphone 5s','ipad air'])].groupby(['language'],as_index=False)['user_id'].nunique()\n",
        "\n",
        "total_users.merge(apple_users,on='language',how='left').sort_values(by='user_id_x',ascending=False).fillna(0)"
      ],
      "metadata": {
        "id": "oGGkmI5nQco3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No Order Customers `Instacart`\n",
        "\n",
        "Identify customers who did not place an order between 2019-02-01 and 2019-03-01.\n",
        "\n",
        "\n",
        "Include:\n",
        "\n",
        "\n",
        "•    Customers who placed orders only outside this date range.\n",
        "•    Customers who never placed any orders."
      ],
      "metadata": {
        "id": "bnuqETewQgHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "custs = orders[((orders['order_date'] >= '2019-02-01') & (orders['order_date'] <= '2019-03-01'))]['cust_id'].values\n",
        "\n",
        "df = customers.merge(orders,left_on='id',right_on='cust_id',how='left')\n",
        "\n",
        "df[~df['id_x'].isin(custs)]['first_name'].drop_duplicates()"
      ],
      "metadata": {
        "id": "r97MYZWqQdsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number Of Units Per Nationality `Airbnb`\n",
        "\n",
        "We have data on rental properties and their owners. Write a query that figures out how many different apartments (use unit_id) are owned by people under 30, broken down by their nationality. We want to see which nationality owns the most apartments, so make sure to sort the results accordingly."
      ],
      "metadata": {
        "id": "9-vJZqVPQzOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = airbnb_units.merge(airbnb_hosts,on='host_id').query('unit_type == \"Apartment\" & age < 30')\n",
        "\n",
        "df.groupby('nationality',as_index=False)['unit_id'].nunique()"
      ],
      "metadata": {
        "id": "sbs8rGBhQ1h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ranking Most Active Guests `Airbnb`\n",
        "\n",
        "Identify the most engaged guests by ranking them according to their overall messaging activity. The most active guest, meaning the one who has exchanged the most messages with hosts, should have the highest rank. If two or more guests have the same number of messages, they should have the same rank. Importantly, the ranking shouldn't skip any numbers, even if many guests share the same rank. Present your results in a clear format, showing the rank, guest identifier, and total number of messages for each guest, ordered from the most to least active."
      ],
      "metadata": {
        "id": "xc9UTNUnQ4iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = airbnb_contacts.groupby('id_guest')['n_messages'].sum().sort_values(ascending=False).reset_index()\n",
        "\n",
        "df.insert(0,'rankings',df['n_messages'].rank(ascending=False,method='dense'))\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "gjm6GBQtQ6vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of Streets Per Zip Code `City of San Francisco`\n",
        "\n",
        "Count the number of unique street names for each postal code in the business dataset. Use only the first word of the street name, case insensitive (e.g., \"FOLSOM\" and \"Folsom\" are the same). If the structure is reversed (e.g., \"Pier 39\" and \"39 Pier\"), count them as the same street. Output the results with postal codes, ordered by the number of streets (descending) and postal code (ascending)."
      ],
      "metadata": {
        "id": "RLnHt2NjQ_tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "sf_restaurant_health_violations['streets'] = sf_restaurant_health_violations['business_address'].apply(lambda x : x.split()).apply(lambda x : x[0] if not x[0].isdigit() else x[1]).str.lower()\n",
        "\n",
        "df = sf_restaurant_health_violations.copy()\n",
        "\n",
        "df = df.groupby('business_postal_code',as_index=False)['streets'].nunique()\n",
        "\n",
        "df.sort_values(by=['streets','business_postal_code'],ascending=[False,True])"
      ],
      "metadata": {
        "id": "Ivw2aIo1RCec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta/Facebook Accounts `Meta`\n",
        "\n",
        "Calculate the ratio of accounts closed on January 10th, 2020 using the fb_account_status table."
      ],
      "metadata": {
        "id": "skcZcircRFDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "(fb_account_status[(fb_account_status['status'] == 'closed') & (fb_account_status['status_date'] == '2020-01-10')].shape[0]) / fb_account_status[(fb_account_status['status_date'] == '2020-01-10')].shape[0]"
      ],
      "metadata": {
        "id": "bb1hENGyRHNq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Share of Active Users `Meta`\n",
        "\n",
        "Calculate the percentage of users who are both from the US and have an 'open' status, as indicated in the fb_active_users table."
      ],
      "metadata": {
        "id": "lpNaSPGtRKvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "(fb_active_users.query('country == \"USA\" & status == \"open\"').shape[0]/fb_active_users.shape[0])*100"
      ],
      "metadata": {
        "id": "78NQZKcwRIb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding Purchases `Amazon`\n",
        "\n",
        "Identify returning active users by finding users who made a second purchase within 7 days of any previous purchase. Output a list of these user_id."
      ],
      "metadata": {
        "id": "B8x3tRP0RT6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = amazon_transactions.sort_values(by=['user_id','created_at'])\n",
        "temp = df[['user_id','created_at']].shift(1).rename(columns={'user_id' : 'shifted_id','created_at':'shifted_created'})\n",
        "\n",
        "final = pd.concat([df,temp],axis=1)\n",
        "\n",
        "final = final[final['user_id'] == final['shifted_id']]\n",
        "\n",
        "final['diff'] = (final['created_at'] - final['shifted_created']).astype(str).str.split(' ').str.get(0).astype(int)\n",
        "\n",
        "final[final['diff'] < 8]['user_id'].unique()\n"
      ],
      "metadata": {
        "id": "n4sjkq77RWhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Premium Accounts `Meta`\n",
        "\n",
        "You have a dataset that records daily active users for each premium account. A premium account appears in the data every day as long as it remains premium. However, some premium accounts may be temporarily discounted, meaning they are not actively paying—this is indicated by a final_price of 0.\n",
        "\n",
        "\n",
        "For each of the first 7 available dates in the dataset, count the number of premium accounts that were actively paying on that day. Then, track how many of those same accounts are still premium and actively paying exactly 7 days later, based solely on their status on that 7th day (i.e., both dates must exist in the dataset). Accounts are only counted if they appear in the data on both dates.\n",
        "\n",
        "\n",
        "Output three columns:\n",
        "\n",
        "•   The date of initial calculation.\n",
        "\n",
        "•   The number of premium accounts that were actively paying on that day.\n",
        "\n",
        "•   The number of those accounts that remain premium and are still paying after 7 days."
      ],
      "metadata": {
        "id": "mUUzTpsyRZEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Start writing code\n",
        "premium_accounts_by_day = premium_accounts_by_day[premium_accounts_by_day['final_price'] != 0]\n",
        "df = premium_accounts_by_day[premium_accounts_by_day['entry_date'] < '2022-02-14'].groupby('entry_date').agg(\n",
        "    {'account_id' : ['count',list]}\n",
        "    )\n",
        "\n",
        "df = df.stack(level=0).reset_index().drop(columns='level_1')\n",
        "\n",
        "temp = premium_accounts_by_day[(premium_accounts_by_day['entry_date'] >= '2022-02-14') & (premium_accounts_by_day['entry_date'] <= '2022-02-20')]\n",
        "\n",
        "temp = temp.groupby('entry_date',as_index=False).agg(\n",
        "    {'account_id' : list}\n",
        "    )\n",
        "\n",
        "temp['entry_date'] = temp['entry_date'] - pd.to_timedelta(7, unit='d')\n",
        "df = temp.merge(df,on='entry_date')\n",
        "\n",
        "df['premium_after7'] = df.apply(lambda x : np.intersect1d(x['account_id'],x['list']),axis=1)\n",
        "\n",
        "df['num_paid_after7'] = df['premium_after7'].apply(len)\n",
        "\n",
        "df[['entry_date','count','num_paid_after7']]"
      ],
      "metadata": {
        "id": "FgZtbonrRcdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Election Results `Deloitte`\n",
        "\n",
        "The election is conducted in a city and everyone can vote for one or more candidates, or choose not to vote at all. Each person has 1 vote so if they vote for multiple candidates, their vote gets equally split across these candidates. For example, if a person votes for 2 candidates, these candidates receive an equivalent of 0.5 vote each. Some voters have chosen not to vote, which explains the blank entries in the dataset.\n",
        "\n",
        "\n",
        "Find out who got the most votes and won the election. Output the name of the candidate or multiple names in case of a tie.\n",
        "To avoid issues with a floating-point error you can round the number of votes received by a candidate to 3 decimal places."
      ],
      "metadata": {
        "id": "JzkYhHUhRihm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = voting_results.dropna()\n",
        "\n",
        "votes = (1/df['voter'].value_counts()).reset_index()\n",
        "\n",
        "df.merge(votes,on='voter').groupby('candidate')['count'].sum().sort_values(ascending=False).head(1).reset_index()['candidate']"
      ],
      "metadata": {
        "id": "i7PeIVvlRk83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flags per Video `Netflix`\n",
        "\n",
        "For each video, find how many unique users flagged it. A unique user can be identified using the combination of their first name and last name. Do not consider rows in which there is no flag ID."
      ],
      "metadata": {
        "id": "89Vxvy1WRoIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = user_flags.dropna(subset='flag_id')\n",
        "\n",
        "df['name'] = df['user_firstname'].fillna('a') + ' ' + df['user_lastname']\n",
        "\n",
        "df.groupby('video_id',as_index=False)['name'].nunique()"
      ],
      "metadata": {
        "id": "hGZEfIZORqIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User with Most Approved Flags `Google`\n",
        "\n",
        "Which user flagged the most distinct videos that ended up approved by YouTube? Output, in one column, their full name or names in case of a tie. In the user's full name, include a space between the first and the last name."
      ],
      "metadata": {
        "id": "JHariOL0Rsmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = flag_review[flag_review['reviewed_outcome'] == 'APPROVED'].merge(user_flags,on='flag_id')\n",
        "\n",
        "df['name'] = df['user_firstname'] + ' ' + df['user_lastname']\n",
        "\n",
        "df = df.groupby('name',as_index=False)['video_id'].nunique().sort_values(by='video_id',ascending=False)\n",
        "\n",
        "df[df['video_id'] == df['video_id'].max()]['name']"
      ],
      "metadata": {
        "id": "VeTQ8A6cRugC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Students At Median Writing `General Assembly`\n",
        "\n",
        "Identify the IDs of students who scored exactly at the median for the SAT writing section."
      ],
      "metadata": {
        "id": "0g_jwZaaRxGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "sat_scores[sat_scores['sat_writing'] == sat_scores['sat_writing'].median()]['student_id']"
      ],
      "metadata": {
        "id": "TSl_IZhsRzn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Host Popularity Rental Prices `Airbnb`\n",
        "\n",
        "You are given a table named airbnb_host_searches that contains data for rental property searches made by users. Determine the minimum, average, and maximum rental prices for each popularity-rating bucket. A popularity-rating bucket should be assigned to every record based on its number_of_reviews (see rules below).\n",
        "\n",
        "\n",
        "The host’s popularity rating is defined as below:\n",
        "-   0 reviews: \"New\"\n",
        "-   1 to 5 reviews: \"Rising\"\n",
        "-   6 to 15 reviews: \"Trending Up\"\n",
        "-   16 to 40 reviews: \"Popular\"\n",
        "-   More than 40 reviews: \"Hot\"\n",
        "\n",
        "\n",
        "Tip: The id column in the table refers to the search ID.\n",
        "\n",
        "\n",
        "Output host popularity rating and their minimum, average and maximum rental prices. Order the solution by the minimum price."
      ],
      "metadata": {
        "id": "AXwH76f8R1I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "\n",
        "def buckets(x):\n",
        "    if x > 40:\n",
        "        return 'Hot'\n",
        "    elif x > 15:\n",
        "        return 'Popular'\n",
        "    elif x > 5:\n",
        "        return 'Trending Up'\n",
        "    elif x > 0:\n",
        "        return 'Rising'\n",
        "    else:\n",
        "        return 'New'\n",
        "\n",
        "airbnb_host_searches['pop'] = airbnb_host_searches['number_of_reviews'].apply(buckets)\n",
        "\n",
        "df = airbnb_host_searches.groupby('pop').agg(\n",
        "    {\n",
        "        'price' : ['min','mean','max']\n",
        "    }\n",
        "    ).stack(level=0).reset_index().drop(columns='level_1').sort_values(by='min')"
      ],
      "metadata": {
        "id": "RLeHJuyPR9sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 10 Songs 2010 `Spotify`\n",
        "\n",
        "Find the top 10 ranked songs in 2010. Output the rank, group name, and song name, but do not show the same song twice. Sort the result based on the rank in ascending order."
      ],
      "metadata": {
        "id": "jNGtlom5SA2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = billboard_top_100_year_end.query('year == 2010').groupby('song_name',as_index=False).agg(\n",
        "    {\n",
        "        'year_rank' : 'max',\n",
        "        'group_name' : 'unique'\n",
        "    }).sort_values(by='year_rank')\n",
        "\n",
        "df['group_name'] = df['group_name'].apply(lambda x : ''.join(x))\n",
        "\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "YjX1ChN7SC9j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}