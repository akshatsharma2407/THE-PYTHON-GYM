{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Practice this question here - https://platform.stratascratch.com/coding?is_freemium=1&difficulties=3&code_type=2"
      ],
      "metadata": {
        "id": "s-70ntMqZBjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consecutive Days `Netflix`\n",
        "\n",
        "Find all the users who were active for 3 consecutive days or more."
      ],
      "metadata": {
        "id": "-AhgKRQUVezr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zltFmc0XVVTQ"
      },
      "outputs": [],
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "sf_events.head()\n",
        "\n",
        "def is_cons(user):\n",
        "    temp = sf_events[sf_events['user_id'] == user]\n",
        "    temp.sort_values(by='record_date',inplace=True)\n",
        "    temp['prev'] = temp['record_date'].shift(1)\n",
        "    temp['prev'] = (temp['record_date'] - temp['prev']).dt.days\n",
        "    count = 0\n",
        "    for i in temp['prev']:\n",
        "        if i == 1:\n",
        "            count += 1\n",
        "            if count == 2:\n",
        "                return True\n",
        "        else:\n",
        "            count = 0\n",
        "    return False\n",
        "\n",
        "sf_events[sf_events['user_id'].apply(is_cons)]['user_id'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Selling Item `Ebay`\n",
        "\n",
        "Find the best-selling item for each month (no need to separate months by year).\n",
        "\n",
        "The best-selling item is determined by the highest total sales amount, calculated as: total_paid = unitprice * quantity. Output the month, description of the item, and the total amount paid."
      ],
      "metadata": {
        "id": "YkTdxNfZVkLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "online_retail['month_no'] = online_retail['invoicedate'].dt.month\n",
        "online_retail['total'] = online_retail['quantity'] * online_retail['unitprice']\n",
        "\n",
        "temp = online_retail.groupby(['month_no','stockcode'],as_index=False)['total'].sum()\n",
        "\n",
        "temp.sort_values(by=['month_no','total'],ascending=[True,False]).drop_duplicates(subset='month_no').merge(online_retail,on='stockcode')[['month_no_x','description','total_x']].rename(columns={\n",
        "    'month_no_x' : 'month',\n",
        "    'total_x' : 'total_paid'\n",
        "}).drop_duplicates()"
      ],
      "metadata": {
        "id": "ePIVCycXVmPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rank Variance Per Country `Meta`\n",
        "\n",
        "Compare the total number of comments made by users in each country between December 2019 and January 2020. For each month, rank countries by total comments using dense ranking (i.e., avoid gaps between ranks) in descending order. Then, return the names of the countries whose rank improved from December to January."
      ],
      "metadata": {
        "id": "D219bdWmVsH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "fb_comments_count['months'] = fb_comments_count['created_at'].dt.month\n",
        "\n",
        "jan = fb_comments_count.query('months == 1')\n",
        "\n",
        "dec = fb_comments_count.query('months == 12')\n",
        "\n",
        "dec = dec.merge(fb_active_users,on='user_id').groupby('country',as_index=False)['number_of_comments'].sum()\n",
        "\n",
        "dec['rank'] = dec['number_of_comments'].rank(ascending=False,method='dense')\n",
        "\n",
        "jan = jan.merge(fb_active_users,on='user_id').groupby('country',as_index=False)['number_of_comments'].sum()\n",
        "\n",
        "jan['rank'] = jan['number_of_comments'].rank(ascending=False,method='dense')\n",
        "\n",
        "dec.merge(jan,on='country').query('rank_y < rank_x')['country']"
      ],
      "metadata": {
        "id": "_RXeoMTVVzhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor Rating Difference Analysis `Google`\n",
        "\n",
        "You are given a dataset of actors and the films they have been involved in, including each film's release date and rating. For each actor, calculate the difference between the rating of their most recent film and their average rating across all previous films (the average rating excludes the most recent one).\n",
        "\n",
        "\n",
        "Return a list of actors along with their average lifetime rating, the rating of their most recent film, and the difference between the two ratings. If an actor has only one film, return 0 for the difference and their only film’s rating for both the average and latest rating fields."
      ],
      "metadata": {
        "id": "Nei_ediGV2eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Start writing code\n",
        "latest = actor_rating_shift.sort_values(by=['actor_name','release_date'],ascending=[True,False]).drop_duplicates(subset='actor_name')\n",
        "\n",
        "\n",
        "def islatest(x):\n",
        "    name = x['actor_name']\n",
        "    date = x['release_date']\n",
        "    if len(actor_rating_shift[actor_rating_shift['actor_name'] == name].values) == 1:\n",
        "        return True\n",
        "\n",
        "    if (name in latest['actor_name'].values) & (date in latest['release_date'].values):\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "avg = actor_rating_shift[actor_rating_shift.apply(islatest,axis=1)].groupby('actor_name',as_index=False)['film_rating'].mean()\n",
        "\n",
        "df = latest.merge(avg,on='actor_name',how='left')[['actor_name','film_rating_y','film_rating_x']].rename(columns={'film_rating_x' : 'latest_rating', 'film_rating_y' : 'avg_rating'})\n",
        "\n",
        "df\n",
        "\n",
        "\n",
        "df['rating_difference'] = df['latest_rating'] - df['avg_rating']\n",
        "\n",
        "df.sort_values(by='actor_name')"
      ],
      "metadata": {
        "id": "zU4ojFyYV54w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Player with Longest Streak `Google`\n",
        "\n",
        "You are given a table of tennis players and their matches that they could either win (W) or lose (L). Find the longest streak of wins. A streak is a set of consecutive won matches of one player. The streak ends once a player loses their next match. Output the ID of the player or players and the length of the streak."
      ],
      "metadata": {
        "id": "erpnIaCLWCIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "def consq(px):\n",
        "    df = players_results.query(f'player_id == {px}').sort_values(by='match_date')\n",
        "    cons = 0\n",
        "    temp_cons = 0\n",
        "    for i in df['match_result'].values :\n",
        "        if i == 'W':\n",
        "            temp_cons += 1\n",
        "        else:\n",
        "            if cons < temp_cons:\n",
        "                cons = temp_cons\n",
        "            temp_cons = 0\n",
        "    return cons\n",
        "\n",
        "# Start writing code\n",
        "df = pd.DataFrame(columns=['player_id','streak'])\n",
        "for j,i in enumerate(players_results['player_id'].unique()):\n",
        "    df.loc[j] = {'player_id' : i , 'streak' : consq(i)}\n",
        "\n",
        "df[df['streak'] == df['streak'].max()]"
      ],
      "metadata": {
        "id": "ijBY_sdFWFpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the genre of the person with the most number of oscar winnings `Netflix`\n",
        "\n",
        "Find the genre of the person with the most number of oscar winnings.\n",
        "If there are more than one person with the same number of oscar wins, return the first one in alphabetic order based on their name. Use the names as keys when joining the tables."
      ],
      "metadata": {
        "id": "m_CvxR3cWJ1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = oscar_nominees.query('winner == True')['nominee'].value_counts().reset_index()\n",
        "\n",
        "name = df[df['nominee'] == df['nominee'].max()].sort_values(by='index').head(1)['index'].values[0]\n",
        "\n",
        "nominee_information[nominee_information['name'] == name]['top_genre']"
      ],
      "metadata": {
        "id": "kVUBKB4uWOTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top Actor Ratings by Genre `Google`\n",
        "\n",
        "Find the top actors based on their average movie rating within the genre they appear in most frequently.\n",
        "\n",
        "-  For each actor, determine their most frequent genre (i.e., the one they’ve appeared in the most).\n",
        "-   If there is a tie in genre count, select the genre where the actor has the highest average rating.\n",
        "-   If there is still a tie in both count and rating, include all tied genres for that actor.\n",
        "\n",
        "\n",
        "Rank all resulting actor + genre pairs in descending order by their average movie rating.\n",
        "-  Return all pairs that fall within the top 3 ranks (not simply the top 3 rows), including ties.\n",
        "-  Do not skip rank numbers — for example, if two actors are tied at rank 1, the next rank is 2 (not 3)."
      ],
      "metadata": {
        "id": "xiLiaNDmWRHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = top_actors_rating.groupby(['actor_name','genre'],as_index=False).agg(\n",
        "    {\n",
        "        'movie_title' : 'count',\n",
        "        'movie_rating' : 'mean'\n",
        "    })\n",
        "\n",
        "max_count = df.groupby('actor_name',as_index=False)['movie_title'].max()\n",
        "\n",
        "df = df.merge(max_count,on='actor_name')\n",
        "\n",
        "df = df[df['movie_title_x'] == df['movie_title_y']]\n",
        "\n",
        "max_rating = df.groupby('actor_name',as_index=False)['movie_rating'].max()\n",
        "\n",
        "df = df.merge(max_rating,on='actor_name')\n",
        "\n",
        "df = df[df['movie_rating_x'] == df['movie_rating_y']]\n",
        "\n",
        "avg_rating = top_actors_rating.groupby(['actor_name','genre'],as_index=False)['movie_rating'].mean()\n",
        "\n",
        "df = df.merge(avg_rating,on=['actor_name','genre']).sort_values(by='movie_rating',ascending=False)[['actor_name','genre','movie_rating']]\n",
        "\n",
        "df['movie_rank'] = df['movie_rating'].rank(ascending=False,method='first')\n",
        "\n",
        "df.query('movie_rank < 6').drop(columns='movie_rank')"
      ],
      "metadata": {
        "id": "SSkCKz1aWXZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monthly Percentage Difference `Amazon`\n",
        "\n",
        "Given a table of purchases by date, calculate the month-over-month percentage change in revenue. The output should include the year-month date (YYYY-MM) and percentage change, rounded to the 2nd decimal point, and sorted from the beginning of the year to the end of the year.\n",
        "The percentage change column will be populated from the 2nd month forward and can be calculated as ((this month's revenue - last month's revenue) / last month's revenue)*100."
      ],
      "metadata": {
        "id": "j7tAEiBEWbXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "sf_transactions['month'] = sf_transactions['created_at'].dt.to_period('M')\n",
        "\n",
        "df = sf_transactions.groupby(['month'],as_index=False)['value'].sum()\n",
        "\n",
        "df['shifted'] = df['value'].shift(1)\n",
        "\n",
        "df['revenue_diff_pct'] = ((df['value'] - df['shifted'])/df['shifted'])*100\n",
        "\n",
        "df.drop(columns=['value','shifted'])"
      ],
      "metadata": {
        "id": "22MyJGCQWdeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customer Tracking `Shopify`\n",
        "\n",
        "Given the users' sessions logs on a particular day, calculate how many hours each user was active that day.\n",
        "\n",
        "\n",
        "Note: The session starts when state=1 and ends when state=0."
      ],
      "metadata": {
        "id": "LtHt7gNEWfwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "cust_tracking['shifted'] = cust_tracking.groupby('cust_id')['timestamp'].shift(1)\n",
        "df = cust_tracking.query('state == 0')\n",
        "\n",
        "df['delta'] = df['timestamp'] - df['shifted']\n",
        "\n",
        "final = df.groupby(['cust_id'],as_index=False)['delta'].sum()\n",
        "\n",
        "final['total_hours'] = final['delta']/3600\n",
        "\n",
        "final[['cust_id','total_hours']]"
      ],
      "metadata": {
        "id": "LCGqykv7Wj9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Streaks `Linkedin`\n",
        "\n",
        "Provided a table with user id and the dates they visited the platform, find the top 3 users with the longest continuous streak of visiting the platform as of August 10, 2022. Output the user ID and the length of the streak.\n",
        "\n",
        "\n",
        "In case of a tie, display all users with the top three longest streaks."
      ],
      "metadata": {
        "id": "413hR1EjWoEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = user_streaks.query(\"date_visited < '2022-08-11'\").sort_values(by=['user_id','date_visited']).drop_duplicates()\n",
        "\n",
        "df['prev'] = df.groupby('user_id')['date_visited'].shift(1)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df['delta'] = ((df['date_visited'] - df['prev'])/3600)/24\n",
        "\n",
        "df['delta'] = df['delta'].dt.seconds\n",
        "\n",
        "def streak(uid):\n",
        "    values = df[df['user_id'] == uid]['delta'].values\n",
        "    stk = 1\n",
        "    for i in values:\n",
        "        if i == 1:\n",
        "            stk += 1\n",
        "        else:\n",
        "            stk = 1\n",
        "    return stk\n",
        "\n",
        "final = pd.DataFrame(columns=['user_id','streak_length'])\n",
        "\n",
        "l = []\n",
        "for i in df['user_id'].unique():\n",
        "    l.append(streak(i))\n",
        "\n",
        "final['user_id'] = df['user_id'].unique()\n",
        "final['streak_length'] = l\n",
        "\n",
        "final = final.sort_values(by='streak_length',ascending=False)\n",
        "\n",
        "final['ranks'] = final['streak_length'].rank(ascending=False,method='dense')\n",
        "final.query('ranks < 4')[['user_id','streak_length']]"
      ],
      "metadata": {
        "id": "bOZ1eBJdWqQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Marketing Campaign Success [Advanced] `ActiveCampaign`\n",
        "\n",
        "You have the marketing_campaign table, which records in-app purchases by users. Users making their first in-app purchase enter a marketing campaign, where they see call-to-actions for more purchases. Find how many users made additional purchases due to the campaign's success.\n",
        "\n",
        "\n",
        "The campaign starts one day after the first purchase. Users with only one or multiple purchases on the first day do not count, nor do users who later buy only the same products from their first day."
      ],
      "metadata": {
        "id": "fESRd7sCWt2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "count = 0\n",
        "for i in marketing_campaign['user_id'].unique():\n",
        "    df = marketing_campaign.query(f'user_id == {i}')\n",
        "    product = df[df['created_at'] == df['created_at'].min()]['product_id'].values\n",
        "    camp = df[df['created_at'] != df['created_at'].min()]\n",
        "    val = np.setdiff1d(camp['product_id'].unique(),product)\n",
        "    if len(val) > 0:\n",
        "        count += 1\n",
        "\n",
        "pd.DataFrame([count],columns=['user_count'])"
      ],
      "metadata": {
        "id": "VzUmEaZWWxVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reviewed flags of top videos `Google`\n",
        "\n",
        "For the video (or videos) that received the most user flags, how many of these flags were reviewed by YouTube? Output the video ID and the corresponding number of reviewed flags.\n"
      ],
      "metadata": {
        "id": "__L2nVTlW01F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = user_flags.merge(flag_review,on='flag_id')\n",
        "\n",
        "temp = df['video_id'].value_counts().reset_index()\n",
        "\n",
        "df[df['video_id'].isin(temp[temp['video_id'] == temp['video_id'].max()]['index'].values)].groupby('video_id')['reviewed_by_yt'].sum().reset_index()"
      ],
      "metadata": {
        "id": "FIlGH1EFW3VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top 5 States With 5 Star Businesses `Yelp`\n",
        "\n",
        "Find the top 5 states with the most 5 star businesses. Output the state name along with the number of 5-star businesses and order records by the number of 5-star businesses in descending order. In case there are ties in the number of businesses, return all the unique states. If two states have the same result, sort them in alphabetical order."
      ],
      "metadata": {
        "id": "WlWMUWCiW5Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "df = yelp_business.query('stars == 5')['state'].value_counts().reset_index()\n",
        "df['rank'] = df['state'].rank(ascending=False,method='dense')\n",
        "df.query('rank < 6')[['index','state']]"
      ],
      "metadata": {
        "id": "wtayGbAMW9GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## City With Most Amenities `Airbnb`\n",
        "\n",
        "You're given a dataset of searches for properties on Airbnb. For simplicity, let's say that each search result (i.e., each row) represents a unique host. Find the city with the most amenities across all their host's properties. Output the name of the city."
      ],
      "metadata": {
        "id": "ZCCavytbXIDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "airbnb_search_details['amenities'] = airbnb_search_details['amenities'].str[1:-1].str.replace('\"','').str.lower().str.strip().str.split(',')\n",
        "\n",
        "df = airbnb_search_details.groupby('city',as_index=False)['amenities'].sum()\n",
        "\n",
        "df['l'] = df['amenities'].str.len()\n",
        "\n",
        "df.sort_values(by='l',ascending=False)['city'].head(1)"
      ],
      "metadata": {
        "id": "Pb-_EJ83XKG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Popularity Percentage `Meta`\n",
        "\n",
        "Find the popularity percentage for each user on Meta/Facebook. The dataset contains two columns, user1 and user2, which represent pairs of friends. Each row indicates a mutual friendship between user1 and user2, meaning both users are friends with each other. A user's popularity percentage is calculated as the total number of friends they have (counting connections from both user1 and user2 columns) divided by the total number of unique users on the platform. Multiply this value by 100 to express it as a percentage.\n",
        "\n",
        "\n",
        "Output each user along with their calculated popularity percentage. The results should be ordered by user ID in ascending order."
      ],
      "metadata": {
        "id": "d7WK3kEVXNG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Start writing code\n",
        "x= facebook_friends['user1'].value_counts().reset_index()\n",
        "y= facebook_friends['user2'].value_counts().reset_index()\n",
        "\n",
        "df = x.merge(y,left_on='user1',how='outer',right_on='user2')\n",
        "\n",
        "total = len(np.union1d(facebook_friends['user1'].unique(),facebook_friends['user2'].unique()))\n",
        "\n",
        "df['user'] = df.apply(lambda x : x['user1'] if not pd.isnull(x['user1']) else x['user2'], axis=1)\n",
        "\n",
        "df = df.fillna(0)\n",
        "\n",
        "df['friends'] = df['count_x'] + df['count_y']\n",
        "\n",
        "df['popularity_percent'] = (df['friends']/total)*100\n",
        "\n",
        "df[['user','popularity_percent']]"
      ],
      "metadata": {
        "id": "q-gk4aVmXPyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top Percentile Fraud `Google`\n",
        "\n",
        "We want to identify the most suspicious claims in each state. We'll consider the top 5 percentile of claims with the highest fraud scores in each state as potentially fraudulent.\n",
        "\n",
        "\n",
        "Your output should include the policy number, state, claim cost, and fraud score."
      ],
      "metadata": {
        "id": "z1kppEXiXS6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Start writing code\n",
        "df = fraud_score.sort_values(by=['state','fraud_score'],ascending=[True,False])\n",
        "\n",
        "per = pd.DataFrame(columns=['state','percentile'])\n",
        "\n",
        "l = []\n",
        "\n",
        "for i,j in df.groupby('state',as_index=False):\n",
        "    l.append([i,j['fraud_score'].quantile(0.95)])\n",
        "\n",
        "per[['state','percentile']] = l\n",
        "\n",
        "df.merge(per,on='state').query('fraud_score >= percentile').drop(columns='percentile')\n"
      ],
      "metadata": {
        "id": "Cw6ZaXSIXViT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cookbook Recipes `Ebay`\n",
        "\n",
        "You are given a table containing recipe titles and their corresponding page numbers from a cookbook. Your task is to format the data to represent how recipes are distributed across double-page spreads in the book.\n",
        "\n",
        "\n",
        "Each spread consists of two pages:\n",
        "\n",
        "\n",
        "-   The left page (even-numbered) and its corresponding recipe title (if any).\n",
        "-   The right page (odd-numbered) and its corresponding recipe title (if any).\n",
        "\n",
        "\n",
        "The output table should contain the following three columns:\n",
        "\n",
        "\n",
        "-   left_page_number – The even-numbered page that starts each double-page spread.\n",
        "-   left_title – The title of the recipe on the left page (if available).\n",
        "-   right_title – The title of the recipe on the right page (if available).\n",
        "\n",
        "\n",
        "For the  k-th  row (starting from 0):\n",
        "\n",
        "\n",
        "-   The  left_page_number  should be $2 * k$.\n",
        "-   The  left_title  should be the title from page $2 * k$, or NULL if there is no recipe on that page.\n",
        "-   The  right_title  should be the title from page $2 * k + 1$, or NULL if there is no recipe on that page.\n",
        "\n",
        "\n",
        "Each page contains at most one recipe and  if a page does not contain a recipe, the corresponding title should be NULL. Page 0 (the inside cover) is always empty and included in the output. The table should ensure that all pages up to the maximum recorded page number are included, even if they contain"
      ],
      "metadata": {
        "id": "xqubFF8OXXHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "\n",
        "l = [i for i in range(0,cookbook_titles.shape[0]*2,2)]\n",
        "df = pd.DataFrame(l,columns=['page_number'])\n",
        "\n",
        "odd = cookbook_titles[cookbook_titles['page_number']%2 != 0]\n",
        "odd['page_number'] = odd['page_number'] - 1\n",
        "\n",
        "even = cookbook_titles[cookbook_titles['page_number']%2 == 0]\n",
        "\n",
        "df.merge(even,on='page_number',how='left').merge(odd,on='page_number',how='left')"
      ],
      "metadata": {
        "id": "B7sM4YnaXe-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retention Rate `Meta`\n",
        "\n",
        "You are given a dataset that tracks user activity. The dataset includes information about the date of user activity, the account_id associated with the activity, and the user_id of the user performing the activity. Each row in the dataset represents a user’s activity on a specific date for a particular account_id.\n",
        "\n",
        "\n",
        "Your task is to calculate the monthly retention rate for users for each account_id for December 2020 and January 2021. The retention rate is defined as the percentage of users active in a given month who have activity in any future month.\n",
        "\n",
        "\n",
        "For instance, a user is considered retained for December 2020 if they have activity in December 2020 and any subsequent month (e.g., January 2021 or later). Similarly, a user is retained for January 2021 if they have activity in January 2021 and any later month (e.g., February 2021 or later).\n",
        "\n",
        "\n",
        "The final output should include the account_id and the ratio of the retention rate in January 2021 to the retention rate in December 2020 for each account_id. If there are no users retained in December 2020, the retention rate ratio should be set to 0."
      ],
      "metadata": {
        "id": "YkhWgm8rXpU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df12 = sf_events[sf_events['record_date'].dt.month == 12]\n",
        "\n",
        "df1 = sf_events[sf_events['record_date'].dt.month == 1]\n",
        "\n",
        "df_dec = sf_events.query('record_date > \"2020-12-31\"').merge(df12,on=['account_id','user_id'])\n",
        "\n",
        "df_jan = sf_events.query('record_date > \"2021-01-31\"').merge(df1,on=['account_id','user_id'])\n",
        "\n",
        "dec_retend = df_dec.groupby(['account_id'])['user_id'].nunique() / df12.groupby(['account_id'])['user_id'].nunique()\n",
        "\n",
        "jan_retend = df_jan.groupby(['account_id'])['user_id'].nunique() / df1.groupby(['account_id'])['user_id'].nunique()\n",
        "\n",
        "jan_retend.fillna(0,inplace=True)\n",
        "\n",
        "(jan_retend/dec_retend).reset_index()"
      ],
      "metadata": {
        "id": "FJsExLosXsr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting Instances in Text `Google`\n",
        "\n",
        "Find the number of times the exact words bull and bear appear in the contents column.\n",
        "\n",
        "\n",
        "Count all occurrences, even if they appear multiple times within the same row. Matches should be case-insensitive and only count exact words, that is, exclude substrings like bullish or bearing.\n",
        "\n",
        "\n",
        "Output the word (bull or bear) and the corresponding number of occurrences."
      ],
      "metadata": {
        "id": "ojQ8GhAqXv-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Start writing code\n",
        "google_file_store['contents'] = google_file_store['contents'].str.lower().str.split(' ')\n",
        "bull = google_file_store['contents'].apply(lambda x : sum([True if i == 'bull' else False for i in x])).sum()\n",
        "\n",
        "bear = google_file_store['contents'].apply(lambda x : sum([True if i == 'bear' else False for i in x])).sum()\n",
        "\n",
        "pd.DataFrame([['bull',bull],['bear',bear]],columns=['word','netry'])"
      ],
      "metadata": {
        "id": "1Jq5bBdgXyS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most Popular Client For Calls `Apple`\n",
        "\n",
        "Select the most popular client_id based on the number of users who individually have at least 50% of their events from the following list: 'video call received', 'video call sent', 'voice call received', 'voice call sent'."
      ],
      "metadata": {
        "id": "XAGwIB70X15Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Start writing code\n",
        "l = ['video call received', 'video call sent', 'voice call received', 'voice call sent']\n",
        "df = fact_events.groupby('user_id',as_index=False).agg(\n",
        "    {'event_type' : set}\n",
        "    )\n",
        "\n",
        "df['event_type'] = df['event_type'].apply(lambda x : list(x))\n",
        "\n",
        "df['common'] = df.apply(lambda x : np.intersect1d(x['event_type'],l),axis=1)\n",
        "\n",
        "df['fifty'] = df['common'].apply(len)/df['event_type'].apply(len)\n",
        "\n",
        "df = df.query('fifty >= 0.5')\n",
        "\n",
        "df.merge(fact_events,on='user_id').groupby('client_id',as_index=False)['user_id'].nunique()['client_id']"
      ],
      "metadata": {
        "id": "Ugv5cU1xX3yM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}